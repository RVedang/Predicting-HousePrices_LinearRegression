{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-christopher",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reading the data\n",
    "df = pd.read_csv(\"FoDS-Assignment-2.csv\")\n",
    "\n",
    "# shuffle the DataFrame rows\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling missing values-----dropping rows method\n",
    "\n",
    "df = df.dropna(axis = 0)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_list = [\"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\", \"floors\", \"waterfront\", \"view\", \"condition\", \"grade\", \"sqft_above\", \"sqft_basement\", \"sqft_living15\", \"sqft_lot15\"]\n",
    "\n",
    "#Feature Scaling\n",
    "#normalising the data values\n",
    "df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset into training data and testing data\n",
    "splitData = int(0.7*len(X))\n",
    "train_X, test_X, train_y, test_y = X[:splitData], X[splitData:], y[:splitData], y[splitData:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = list(df.columns.values)\n",
    "for x in head:\n",
    "    q90, q10 = np.percentile(df.loc[:, x],[90, 10])\n",
    "    IQR = q90-q10\n",
    " \n",
    "    max = q90 + (2 * IQR)\n",
    "    min = q10 - (2 * IQR)\n",
    " \n",
    "    df.loc[df[x] < min, x] = np.nan\n",
    "    df.loc[df[x] > max, x] = np.nan\n",
    "\n",
    "print(df.isnull().sum())\n",
    "df = df.dropna(axis = 0)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "WeightV = np.zeros(14)\n",
    "for n in range(14):\n",
    "    WeightV[n] = np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the weights based on the training data\n",
    "def fit(X, Y, iters, learning_rate, F_selected, F_trial):\n",
    "    for n in range(14):\n",
    "        WeightV[n] = np.random.randn()\n",
    "    for itr in range(iters):\n",
    "        sumItrError = 0    \n",
    "        for z in range(len(X)):        # each row in input data\n",
    "            dataP_error = 0            # calculating error in each data point\n",
    "            \n",
    "            for m in range(13):\n",
    "                dataP_error += (F_selected[m] + F_trial[m]) * WeightV[m+1] * X[z][m]    # summation of (w1*x1 + w2*x2 + w3*x3 + w4*x4 ...)\n",
    "                \n",
    "            dataP_error += WeightV[0]\n",
    "            dataP_error -= Y[z]      # (w0 + w1*x1 + w2*x1^2 + w3*x1*x2 + w4*x2^2 ...) - yn\n",
    "\n",
    "            # for each parameter(w0, w1, w2,...)    \n",
    "            for m in range(14):                   \n",
    "                if(m == 0):\n",
    "                    WeightV[m] -= (learning_rate/len(X)) * dataP_error\n",
    "                else:\n",
    "                    WeightV[m] -= (learning_rate/len(X)) * dataP_error * X[z][m-1]   # calculating w0, w1, w2,... for each iteration\n",
    "        \n",
    "            dataP_error = (dataP_error**2)\n",
    "            sumItrError += dataP_error/(2*len(Y))\n",
    "            \n",
    "        sumItrError = (sumItrError)**0.5\n",
    "    return sumItrError    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, F_selected, F_trial):\n",
    "    sumItrError = 0\n",
    "    for z in range(len(X)):        # each row in input data\n",
    "        dataP_error = 0            # calculating error in each data point\n",
    "            \n",
    "        for m in range(13):\n",
    "            dataP_error += (F_selected[m] + F_trial[m]) * WeightV[m+1] * X[z][m]    # summation of (w1*x1 + w2*x2 + w3*x3 + w4*x4 ...)\n",
    "        dataP_error += WeightV[0]\n",
    "        \n",
    "        dataP_error -= Y[z]      # (w0 + w1*x1 + w2*x1^2 + w3*x1*x2 + w4*x2^2 ...) - yn\n",
    "        dataP_error = (dataP_error**2)\n",
    "        sumItrError += dataP_error/(2*len(X))\n",
    "            \n",
    "    sumItrError = (sumItrError)**0.5\n",
    "    return sumItrError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_selected = np.zeros(13)\n",
    "F_trial = np.zeros(13)\n",
    "finalFeatures = np.zeros(13)\n",
    "finalMinE = float('inf')\n",
    "\n",
    "for i in range(13):\n",
    "    minErrorIn_i = float('inf')\n",
    "    for j in range(13):\n",
    "        if(F_selected[j]==1):\n",
    "            continue\n",
    "        F_trial[j] = 1\n",
    "        error_j = fit(train_X, train_y, 10, 0.01, F_selected, F_trial)\n",
    "        print(error_j)\n",
    "        if(error_j < minErrorIn_i):\n",
    "            minErrorIn_i = error_j\n",
    "            minIndex = j\n",
    "        F_trial[j] = 0\n",
    "    F_selected[minIndex] = 1 \n",
    "    if(minErrorIn_i < finalMinE):\n",
    "        finalMinE = minErrorIn_i\n",
    "        finalIndex = i\n",
    "        for u in range(13):\n",
    "            finalFeatures[u] = F_selected[u]\n",
    "    print(\"Minimum error for\", i+1, \"feature(s) is\", minErrorIn_i)\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------------------------------------\")    \n",
    "print(\"Minimum training error is\", finalMinE) \n",
    "print(\"Number of features needed for giving this minimum training error are\", finalIndex+1)   \n",
    "print(\"\\nList of features giving minimum training error -\")\n",
    "for i in range(13):\n",
    "    if(finalFeatures[i]==1):\n",
    "        print(Features_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(train_X, train_y, 500, 1, finalFeatures, F_trial)\n",
    "\n",
    "#making predictions on test data\n",
    "testingE = predict(test_X, test_y, finalFeatures, F_trial)\n",
    "print(\"Minimum testing error considering these\", finalIndex+1, \"features is\", testingE)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
